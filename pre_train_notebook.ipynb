{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training Model Notebook\n",
    "\n",
    "This notebook migrates the code from pre_train.py, maintaining the original logic and structure completely unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third-party Library Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tokenizers\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import gc\n",
    "import json\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Internal Dependencies\n",
    "\n",
    "The following code is migrated from utils.py, dataset.py, and models.py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextGenerator from utils.py\n",
    "import time\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        tokenizer: tokenizers.Tokenizer,\n",
    "        device,\n",
    "        padding_side=\"right\",\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            print(\"DataParallel is used\")\n",
    "            self.model = model.module\n",
    "        else:\n",
    "            self.model = model\n",
    "        self.seq_max_len = self.model.args.seq_max_len\n",
    "        self.padding_side = padding_side\n",
    "        self.tokenizer.enable_padding(direction=padding_side, length=self.seq_max_len)\n",
    "        self.tokenizer.enable_truncation(max_length=self.seq_max_len, direction=padding_side)\n",
    "\n",
    "    def generate(\n",
    "    self,\n",
    "    start_token: str,\n",
    "    gen_seq_len=30,\n",
    "    temperature=0.7,\n",
    "    frequency_penalty=0.1,\n",
    "    top_k=20,\n",
    "    print_out=True,\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            tokens = [start_token]\n",
    "            all_token_ids = self.tokenizer.encode(start_token).ids\n",
    "\n",
    "            for i in range(gen_seq_len):\n",
    "                all_token_ids = self.tokenizer.encode(''.join(tokens)).ids\n",
    "                input_tensor = torch.tensor(all_token_ids).int().unsqueeze(0).to(self.device)\n",
    "                out = self.model(input_tensor)\n",
    "                \n",
    "                if self.padding_side == \"right\":\n",
    "                    logits = out[0, len(tokens)-1, :]  \n",
    "                elif self.padding_side == \"left\":\n",
    "                    logits = out[0, -1, :]\n",
    "                else:\n",
    "                    raise ValueError(\"padding_side must be 'right' or 'left'\")\n",
    "                \n",
    "                if frequency_penalty != 0:\n",
    "                    tokens_tensor = torch.tensor(all_token_ids, device=self.device)\n",
    "                    unique, counts = torch.unique(tokens_tensor, return_counts=True)\n",
    "                    \n",
    "                    penalty = torch.zeros_like(logits)\n",
    "                    penalty[unique] = counts.float() * frequency_penalty\n",
    "                    logits = logits - penalty\n",
    "                \n",
    "                if top_k is not None and top_k > 0:\n",
    "                    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "                    logits[indices_to_remove] = -float('Inf')\n",
    "                \n",
    "                probabilities = F.softmax(logits / temperature, dim=-1)\n",
    "                next_token_id = probabilities.multinomial(num_samples=1).item()\n",
    "                \n",
    "                tokens.append(self.tokenizer.decode([next_token_id], skip_special_tokens=False))\n",
    "                if print_out:\n",
    "                    print(tokens[-1], end=\" \", flush=True)\n",
    "            \n",
    "            return tokens\n",
    "\n",
    "\n",
    "class DebugTimer:\n",
    "    def __init__(self, name=None):\n",
    "        self.start_time = None\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            self.timer_start(self.name)\n",
    "            result = func(*args, **kwargs)\n",
    "            self.timer_stop()\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    def timer_start(self, name=None):\n",
    "        self.start_time = time.perf_counter()\n",
    "        if name is not None:\n",
    "            self.name = name\n",
    "        print(f\"{self.name}:\", end=\"\")\n",
    "\n",
    "    def timer_stop(self):\n",
    "        elapsed_time = round(time.perf_counter() - self.start_time, 4)\n",
    "        print(f\"{elapsed_time}s\")\n",
    "\n",
    "def _format_string(s, length, fill_char=\" \"):\n",
    "    return s.ljust(length, fill_char)\n",
    "\n",
    "def model_structure(model):\n",
    "    print(\"-\" * 90)\n",
    "    print(\n",
    "        \"|\"\n",
    "        + _format_string(\"weight name\", 31)\n",
    "        + \"|\"\n",
    "        + _format_string(\"weight shape\", 42)\n",
    "        + \"|\"\n",
    "        + _format_string(\"number\", 13)\n",
    "        + \"|\"\n",
    "    )\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    total_params = 0\n",
    "    type_size = 1\n",
    "\n",
    "    for key, param in model.named_parameters():\n",
    "        formatted_key = _format_string(key, 30)\n",
    "        shape_str = _format_string(str(param.shape), 40)\n",
    "        param_count = param.numel()\n",
    "        formatted_count = _format_string(str(param_count), 10)\n",
    "\n",
    "        print(f\"| {formatted_key} | {shape_str} | {formatted_count} |\")\n",
    "        total_params += param_count\n",
    "\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"The total number of parameters: {total_params}\")\n",
    "    print(\n",
    "        f\"The parameters of Model {model._get_name()}: {total_params * type_size / 1e6:.4f}M\"\n",
    "    )\n",
    "    print(\"-\" * 90)\n",
    "    return total_params\n",
    "\n",
    "\n",
    "class WarmUpCosineLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, total_epochs, warmup_epochs, min_lr=0, last_epoch=-1):\n",
    "        self.last_epoch = last_epoch\n",
    "        self.total_epochs = total_epochs\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.min_lr = min_lr\n",
    "        super(WarmUpCosineLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            return [\n",
    "                base_lr * (self.last_epoch + 1) / self.warmup_epochs\n",
    "                for base_lr in self.base_lrs\n",
    "            ]\n",
    "        else:\n",
    "            current_epoch = self.last_epoch - self.warmup_epochs\n",
    "            total_cosine_epochs = self.total_epochs - self.warmup_epochs\n",
    "            return [\n",
    "                self.min_lr\n",
    "                + (base_lr - self.min_lr)\n",
    "                * (\n",
    "                    1\n",
    "                    + torch.cos(\n",
    "                        torch.tensor(current_epoch / total_cosine_epochs * torch.pi)\n",
    "                    )\n",
    "                )\n",
    "                / 2\n",
    "                for base_lr in self.base_lrs\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset classes from dataset.py\n",
    "import collections\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "\n",
    "class StreamingTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        tokenizer: tokenizers.Tokenizer,\n",
    "        seq_max_len: int = 192,\n",
    "        downsample: int = 1,\n",
    "        batch: bool = None,\n",
    "        re_tokenize: bool = False,\n",
    "        padding_side: str = \"right\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.re_tokenize = re_tokenize\n",
    "        self.padding_side = padding_side\n",
    "\n",
    "        self.line_offsets = []\n",
    "        self._build_line_index(downsample)\n",
    "\n",
    "    def _build_line_index(self, downsample: int):\n",
    "        with open(self.data_dir, \"rb\") as f:\n",
    "            offset = 0\n",
    "            line_count = 0\n",
    "            while True:\n",
    "                self.line_offsets.append(offset)\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "\n",
    "                offset += len(line)\n",
    "                line_count += 1\n",
    "            self.line_offsets = random.sample(\n",
    "                self.line_offsets, k=int(len(self.line_offsets) * downsample)\n",
    "            )\n",
    "\n",
    "    def pad_seq(\n",
    "        self,\n",
    "        seq: list[int],\n",
    "        max_len: int,\n",
    "        truncation=True,\n",
    "        padding_value=0,\n",
    "        padding_side=\"left\",\n",
    "    ):\n",
    "        if truncation:\n",
    "            if padding_side == \"right\":\n",
    "                seq = seq[:max_len]\n",
    "            elif padding_side == \"left\":\n",
    "                seq = seq[-max_len:]\n",
    "            else:\n",
    "                raise ValueError(\"padding_side must be 'left' or 'right'\")\n",
    "\n",
    "        if len(seq) < max_len:\n",
    "            if padding_side == \"left\":\n",
    "                seq = [padding_value] * (max_len - len(seq)) + seq\n",
    "            elif padding_side == \"right\":\n",
    "                seq = seq + [padding_value] * (max_len - len(seq))\n",
    "            else:\n",
    "                raise ValueError(\"padding_side must be 'left' or 'right'\")\n",
    "\n",
    "        return seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.line_offsets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with open(self.data_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "            f.seek(self.line_offsets[index])\n",
    "            line = f.readline().strip()\n",
    "\n",
    "        if self.re_tokenize:\n",
    "            raw = self.tokenizer.encode(line).ids\n",
    "        else:\n",
    "            raw = self.tokenizer.encode(line.split(\" \"), is_pretokenized=True).ids\n",
    "\n",
    "        raw = self.pad_seq(\n",
    "            raw,\n",
    "            max_len=self.seq_max_len,\n",
    "            truncation=True,\n",
    "            padding_value=0,\n",
    "            padding_side=self.padding_side,\n",
    "        )\n",
    "\n",
    "        raw_tensor = torch.tensor(raw, dtype=torch.long)\n",
    "\n",
    "        return (raw_tensor[:-1].contiguous(), raw_tensor[1:].contiguous())\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, word_counts, specials=[\"<PAD>\", \"<UNK>\"]):\n",
    "        self.stoi = {}\n",
    "        self.itos = []\n",
    "        for special in specials:\n",
    "            self.stoi[special] = len(self.stoi)\n",
    "\n",
    "        for word, _ in word_counts:\n",
    "            self.stoi.setdefault(word, len(self.stoi))\n",
    "\n",
    "        self.itos = list(self.stoi.keys())\n",
    "        self.default_index = self.stoi[\"<UNK>\"]\n",
    "\n",
    "    def __call__(self, word):\n",
    "        return self.stoi.get(word, self.default_index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def build_from_dict(self, word_dict):\n",
    "        self.stoi = word_dict\n",
    "        self.itos = list(self.stoi.keys())\n",
    "        self.default_index = self.stoi[\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model classes from models.py\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class MyLMArgs:\n",
    "    d_model: int\n",
    "    d_inner: int\n",
    "    n_layers: int\n",
    "    vocab_size: int\n",
    "    seq_max_len: int\n",
    "    use_moe: bool = False\n",
    "    n_heads: int = None\n",
    "    n_experts: int = 4\n",
    "    n_experts_per_tok: int = 2\n",
    "    d_conv: int = 3\n",
    "    conv_bias: bool = True\n",
    "    ffn_bias: bool = False\n",
    "    attn_bias: bool = False\n",
    "    d_head: int = 64\n",
    "    dropout: float = 0.1\n",
    "    init_std: float = 0.25\n",
    "    resid_pdrop: float = 0.1\n",
    "    resid_scale: float = 1.0\n",
    "    layer_scale: float = 1.0\n",
    "    use_deepnet_scaling: bool = True\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def _reset_parameters(self, init_std=0.02):\n",
    "        with torch.no_grad():\n",
    "            self.weight.fill_(1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        x = x.to(torch.float32)\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * x.to(input_dtype)\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "        self.pad = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=self.pad,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)[:, :, : -self.pad]\n",
    "\n",
    "class GPT2PositionEmbedding(nn.Module):\n",
    "    def __init__(self, seq_max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_emb = nn.Embedding(seq_max_len, d_model)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self, init_std=0.02):\n",
    "        nn.init.normal_(self.pos_emb.weight, std=init_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "\n",
    "        assert (\n",
    "            seq_len <= self.pos_emb.num_embeddings\n",
    "        ), f\"Sequence length {seq_len} exceeds max length {self.pos_emb.num_embeddings}\"\n",
    "        pos = torch.arange(seq_len).to(x.device)\n",
    "        pos_emb = self.pos_emb(pos)\n",
    "        pos_emb = pos_emb.unsqueeze(0)\n",
    "        return x + pos_emb\n",
    "\n",
    "class MyPositionEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, d_out, d_inner=None):\n",
    "        super().__init__()\n",
    "        if d_inner is None:\n",
    "            d_inner = d_model // 16\n",
    "        self.d_inner = d_inner\n",
    "        self.d_model = d_model\n",
    "        self.gru = nn.GRU(d_inner, d_inner, bias=False, batch_first=True)\n",
    "        self.pos_conv = nn.Conv1d(d_model, d_inner, 1)\n",
    "        self.proj_conv = nn.Conv1d(d_model, d_model - d_inner, 1)\n",
    "        self.linear = nn.Linear(d_model, d_out, bias=False)\n",
    "\n",
    "    def _reset_parameters(self, init_std=0.02):\n",
    "        torch.nn.init.normal_(self.linear.weight, std=init_std)\n",
    "        if hasattr(self, 'pos_conv') and self.pos_conv.weight is not None:\n",
    "            torch.nn.init.normal_(self.pos_conv.weight, std=init_std)\n",
    "        if hasattr(self, 'proj_conv') and self.proj_conv.weight is not None:\n",
    "            torch.nn.init.normal_(self.proj_conv.weight, std=init_std)\n",
    "        if hasattr(self, 'gru'):\n",
    "            for name, param in self.gru.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    torch.nn.init.normal_(param, std=init_std)\n",
    "                elif 'bias' in name:\n",
    "                    torch.nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = x.transpose(1, 2)\n",
    "        x_proj = self.proj_conv(x).transpose(1, 2)\n",
    "        pos = self.pos_conv(x).transpose(1, 2)\n",
    "        pos, _ = self.gru(pos)\n",
    "        x = self.linear(torch.cat([pos, x_proj], dim=-1))\n",
    "\n",
    "        return x + res\n",
    "\n",
    "class ALiBi(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        slopes = torch.Tensor(self._get_slopes(num_heads))\n",
    "        self.register_buffer(\"slopes\", slopes)\n",
    "\n",
    "    def _get_slopes(self, n):\n",
    "        def get_slopes_power_of_2(n):\n",
    "            start = 2 ** (-(2 ** -(math.log2(n) - 3)))\n",
    "            ratio = start\n",
    "            return [start * ratio**i for i in range(n)]\n",
    "\n",
    "        if math.log2(n).is_integer():\n",
    "            return get_slopes_power_of_2(n)\n",
    "        else:\n",
    "            closest_power_of_2 = 2 ** math.floor(math.log2(n))\n",
    "            return (\n",
    "                get_slopes_power_of_2(closest_power_of_2)\n",
    "                + self._get_slopes(2 * closest_power_of_2)[0::2][\n",
    "                    : n - closest_power_of_2\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def forward(self, seq_len, batch_size, device):\n",
    "        context_position = torch.arange(seq_len, device=device)[:, None]\n",
    "        memory_position = torch.arange(seq_len, device=device)[None, :]\n",
    "        relative_position = torch.abs(\n",
    "            context_position - memory_position\n",
    "        )\n",
    "\n",
    "        bias = relative_position[None, ...] * self.slopes[:, None, None]\n",
    "        bias = -bias\n",
    "\n",
    "        bias = bias.repeat(batch_size, 1, 1)\n",
    "        return bias\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: MyLMArgs):\n",
    "        super().__init__()\n",
    "        self.d_model = args.d_model\n",
    "        self.n_heads = args.n_heads or (args.d_model // args.d_head)\n",
    "        self.d_head = args.d_head\n",
    "        self.seq_max_len = args.seq_max_len\n",
    "\n",
    "        self.q_proj = nn.Linear(args.d_model, args.d_model)\n",
    "        self.k_proj = nn.Linear(args.d_model, args.d_model)\n",
    "        self.v_proj = nn.Linear(args.d_model, args.d_model)\n",
    "        self.o_proj = nn.Linear(args.d_model, args.d_model)\n",
    "\n",
    "        self.register_buffer(\"cos_cached\", torch.zeros(1, 1, args.seq_max_len, args.d_head))\n",
    "        self.register_buffer(\"sin_cached\", torch.zeros(1, 1, args.seq_max_len, args.d_head))\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        \n",
    "        self._init_rope()\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _init_rope(self):\n",
    "        d_head_half = self.d_head // 2\n",
    "        inv_freq = 1.0 / (\n",
    "            100 ** (torch.arange(0, d_head_half, dtype=torch.float) / d_head_half)\n",
    "        )\n",
    "\n",
    "        t = torch.arange(self.seq_max_len, dtype=torch.float)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().unsqueeze(0).unsqueeze(0))\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    def _reset_parameters(self, init_std=0.02):\n",
    "        torch.nn.init.normal_(self.q_proj.weight, std=init_std)\n",
    "        torch.nn.init.normal_(self.k_proj.weight, std=init_std)\n",
    "        torch.nn.init.normal_(self.v_proj.weight, std=init_std)\n",
    "        torch.nn.init.normal_(self.o_proj.weight, std=init_std)\n",
    "\n",
    "    def _rotate_half(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = x[..., : x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2 :]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "    def _apply_rotary_pos_emb(\n",
    "        self, q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    "    ):\n",
    "        cos = cos[:, :, : q.size(2), :]\n",
    "        sin = sin[:, :, : q.size(2), :]\n",
    "\n",
    "        q_embed = (q * cos) + (self._rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (self._rotate_half(k) * sin)\n",
    "        return q_embed, k_embed\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_ids=None) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "\n",
    "        cos = self.cos_cached[:, :, :seq_len, :]\n",
    "        sin = self.sin_cached[:, :, :seq_len, :]\n",
    "        q, k = self._apply_rotary_pos_emb(q, k, cos, sin)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.d_head))\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=att.device)).view(1, 1, seq_len, seq_len)\n",
    "        att = att.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "\n",
    "        y = att @ v\n",
    "        y = (\n",
    "            y.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        )\n",
    "\n",
    "        y = self.resid_dropout(self.o_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, args: MyLMArgs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.gate_proj = nn.Linear(args.d_model, args.d_inner, bias=False)\n",
    "        self.up_proj = nn.Linear(args.d_model, args.d_inner, bias=False)\n",
    "        self.down_proj = nn.Linear(args.d_inner, args.d_model, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self, init_std=0.02):\n",
    "        print('on call _reset_parameters')\n",
    "        torch.nn.init.normal_(self.gate_proj.weight, std=init_std)\n",
    "        torch.nn.init.normal_(self.up_proj.weight, std=init_std)\n",
    "        torch.nn.init.normal_(self.down_proj.weight, std=init_std)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_ids=None) -> torch.Tensor:\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "\n",
    "class MoEFFN(nn.Module):\n",
    "    def __init__(self, args: MyLMArgs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.router = nn.Linear(args.d_model, args.n_experts)\n",
    "        self.experts = nn.ModuleList([FFN(args) for _ in range(args.n_experts)])\n",
    "\n",
    "    def _reset_parameters(self, init_std=0.02, ffn_scale=1.0, resid_scale=1.0, n_layers=12):\n",
    "        torch.nn.init.normal_(self.router.weight, std=init_std * 0.1)\n",
    "        if self.router.bias is not None:\n",
    "            nn.init.zeros_(self.router.bias)\n",
    "        for expert in self.experts:\n",
    "            expert._reset_parameters(init_std, ffn_scale, resid_scale, n_layers)\n",
    "\n",
    "    def forward(self, x, token_ids=None):\n",
    "        probs = F.softmax(self.router(x), dim=-1)\n",
    "        top_k_probs, top_k_indices = torch.topk(\n",
    "            probs, self.args.n_experts_per_tok, dim=-1\n",
    "        )\n",
    "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        expert_inputs = x.view(\n",
    "            -1, x.shape[-1]\n",
    "        )\n",
    "        expert_inputs = expert_inputs.repeat_interleave(\n",
    "            self.args.n_experts_per_tok, dim=0\n",
    "        )\n",
    "        flat_top_k_idx = top_k_indices.view(-1)\n",
    "        expert_outputs = torch.zeros_like(expert_inputs)\n",
    "\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            mask = flat_top_k_idx == expert_idx\n",
    "            if mask.any():\n",
    "                expert_outputs[mask] = expert(expert_inputs[mask], token_ids=token_ids)\n",
    "        expert_outputs = (\n",
    "            expert_outputs.view(*top_k_probs.shape, -1) * top_k_probs.unsqueeze(-1)\n",
    "        ).sum(\n",
    "            dim=2\n",
    "        )\n",
    "\n",
    "        return expert_outputs\n",
    "\n",
    "class MyLMDecoderLayer(nn.Module):\n",
    "    def __init__(self, args: MyLMArgs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.attn = Attention(args)\n",
    "        self.mlp = MoEFFN(args) if args.use_moe else FFN(args)\n",
    "        self.input_layernorm = RMSNorm(args.d_model)\n",
    "        self.post_attention_layernorm = RMSNorm(args.d_model)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_ids=None) -> torch.Tensor:\n",
    "        residual = x\n",
    "        x = self.input_layernorm(x)\n",
    "        x = self.attn(x, token_ids=token_ids)\n",
    "        x = residual + x\n",
    "        \n",
    "\n",
    "        residual = x\n",
    "        x = self.post_attention_layernorm(x)\n",
    "        x = self.mlp(x, token_ids=token_ids)\n",
    "        x = residual + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyLM(nn.Module):\n",
    "    def __init__(self, args: MyLMArgs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.token_embedding = nn.Embedding(args.vocab_size, args.d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [MyLMDecoderLayer(args) for _ in range(args.n_layers)]\n",
    "        )\n",
    "\n",
    "        self.norm = RMSNorm(args.d_model)\n",
    "        self.head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, token_ids=None) -> torch.Tensor:\n",
    "        x = self.token_embedding(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, token_ids=token_ids)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training Main Code\n",
    "\n",
    "The following is the core training code migrated from pre_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = DebugTimer()\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    data_dir: str = r\"data_large_ChatML.txt\"\n",
    "    tokenizer_dir: str = r\"bpe_tokenizer_6k_0724_ChatML.json\"\n",
    "    model_save_dir: str = r\"model\\model_state.pth\"\n",
    "    ckpt_save_dir: str = r\"ckpt\\ckpt.pth\"\n",
    "    config_save_dir: str = r\"config.json\"\n",
    "    log_dir: str = r\"logs\"\n",
    "    padding_side = \"left\"\n",
    "\n",
    "    seed: int = 42\n",
    "    epochs: int = 5\n",
    "    batch_size: int = 32\n",
    "    batch_acceleration: int = 4\n",
    "    dataset_downsample: int = 0.008\n",
    "    valset_rate: float = 0.01\n",
    "    val_interval_step: int = 100\n",
    "    seq_max_len=192\n",
    "\n",
    "    learning_rate: float = 5e-3\n",
    "    min_learning_rate: float = 5e-4\n",
    "    warmup_steps: int = 1\n",
    "    use_amp: bool = False\n",
    "\n",
    "    model_args = MyLMArgs(\n",
    "        d_model=256,\n",
    "        d_inner=int(((256 * (8 / 3)) // 64) * 64),\n",
    "        d_head=64,\n",
    "        n_heads=None,\n",
    "        n_layers=1,\n",
    "        vocab_size=None,\n",
    "        seq_max_len=seq_max_len,\n",
    "        use_moe=False,\n",
    "        n_experts=None,\n",
    "        n_experts_per_tok=None,\n",
    "        d_conv = None,\n",
    "        conv_bias = None,\n",
    "        ffn_bias = False,\n",
    "        attn_bias = True,\n",
    "        dropout = 0.1\n",
    "    )\n",
    "\n",
    "    ckpt_interval_step: int = 1000\n",
    "    resume_from: Optional[str] = None\n",
    "\n",
    "class PreTrainer:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self._set_seed()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = tokenizers.Tokenizer.from_file(config.tokenizer_dir)\n",
    "        self.config.model_args.vocab_size = len(self.tokenizer.get_vocab())\n",
    "        self.train_loader, self.val_loader = self._build_dataloader()\n",
    "        self.model = self._build_model().to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        self.optimizer, self.scheduler = self._build_optimizer()\n",
    "        self.scaler = torch.GradScaler(self.device, enabled=config.use_amp)\n",
    "        self.generator = TextGenerator(\n",
    "            self.model, self.tokenizer, self.device, padding_side=config.padding_side\n",
    "        )\n",
    "\n",
    "        self.current_epoch = 0\n",
    "        self.global_step = 0\n",
    "        self.start_epoch = 0\n",
    "        self.current_step = 0\n",
    "        self.start_step = 0\n",
    "        self.train_loss_log = []\n",
    "        self.val_loss_log = []\n",
    "        self.lr_log = []\n",
    "\n",
    "        if config.resume_from is not None:\n",
    "            self.load_checkpoint(config.resume_from)\n",
    "\n",
    "    def _set_seed(self):\n",
    "        random.seed(self.config.seed)\n",
    "        np.random.seed(self.config.seed)\n",
    "        torch.manual_seed(self.config.seed)\n",
    "        torch.cuda.manual_seed(self.config.seed)\n",
    "        torch.cuda.manual_seed_all(self.config.seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = MyLM(self.config.model_args)\n",
    "        return model\n",
    "\n",
    "    def _build_dataloader(self):\n",
    "        dataset = StreamingTextDataset(\n",
    "            self.config.data_dir,\n",
    "            downsample=self.config.dataset_downsample,\n",
    "            seq_max_len=self.config.seq_max_len,\n",
    "            tokenizer=self.tokenizer,\n",
    "            re_tokenize=False,\n",
    "            batch=False,\n",
    "            padding_side=self.config.padding_side,\n",
    "        )\n",
    "        val_dataset_len = int(len(dataset) * self.config.valset_rate)\n",
    "        train_dataset_len = len(dataset) - val_dataset_len\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_dataset_len, val_dataset_len]\n",
    "        )\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=False,\n",
    "            num_workers=4\n",
    "        )\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=False,\n",
    "            num_workers=1\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            amsgrad=False,\n",
    "            betas=(0.85, 0.99),\n",
    "            eps=1e-6,\n",
    "            weight_decay=0.005,\n",
    "        )\n",
    "\n",
    "        scheduler = WarmUpCosineLR(\n",
    "            optimizer,\n",
    "            total_epochs=(\n",
    "                self.config.epochs\n",
    "                * (len(self.train_loader) // self.config.batch_acceleration + 1)\n",
    "            )\n",
    "            + 1,\n",
    "            warmup_epochs=self.config.warmup_steps,\n",
    "            min_lr=self.config.min_learning_rate,\n",
    "        )\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def save_checkpoint(self, path: str, is_final: bool = False):\n",
    "        if isinstance(self.model, nn.DataParallel):\n",
    "            model_state_dict = self.model.module.state_dict()\n",
    "        else:\n",
    "            model_state_dict = self.model.state_dict()\n",
    "        state = {\n",
    "            \"epoch\": self.current_epoch,\n",
    "            \"global_step\": self.global_step,\n",
    "            \"current_step\": self.current_step,\n",
    "            \"model_state_dict\": model_state_dict,\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": self.scheduler.state_dict(),\n",
    "            \"train_loss\": self.train_loss_log[-1] if self.train_loss_log else None,\n",
    "            \"rng_states\": {\n",
    "                \"torch\": torch.get_rng_state(),\n",
    "                \"cuda\": (\n",
    "                    torch.cuda.get_rng_state_all()\n",
    "                    if torch.cuda.is_available()\n",
    "                    else None\n",
    "                ),\n",
    "                \"random\": random.getstate(),\n",
    "                \"numpy\": np.random.get_state(),\n",
    "            },\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "        if is_final:\n",
    "            torch.save(model_state_dict, self.config.model_save_dir)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str):\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "\n",
    "        self.current_epoch = checkpoint[\"epoch\"]\n",
    "        self.global_step = checkpoint[\"global_step\"]\n",
    "        self.start_epoch = checkpoint[\"epoch\"]\n",
    "        self.start_step = checkpoint[\"current_step\"]\n",
    "\n",
    "        rng_states = checkpoint[\"rng_states\"]\n",
    "        torch.set_rng_state(rng_states[\"torch\"])\n",
    "        if rng_states[\"cuda\"] and torch.cuda.is_available():\n",
    "            torch.cuda.set_rng_state_all(rng_states[\"cuda\"])\n",
    "        random.setstate(rng_states[\"random\"])\n",
    "        np.random.set_state(rng_states[\"numpy\"])\n",
    "\n",
    "    def _train_step(self, inputs, targets):\n",
    "        self.model.train()\n",
    "        inputs = inputs.to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "\n",
    "        with torch.autocast(str(self.device), enabled=self.config.use_amp):\n",
    "            output = self.model(inputs)\n",
    "            loss = self.criterion(\n",
    "                output.view(-1, self.config.model_args.vocab_size), targets.view(-1)\n",
    "            )\n",
    "\n",
    "        loss = loss / self.config.batch_acceleration\n",
    "\n",
    "        self.scaler.scale(loss).backward()\n",
    "\n",
    "        if ((self.current_step + 1) % self.config.batch_acceleration == 0) or (\n",
    "            self.current_step + 1 == len(self.train_loader)\n",
    "        ):\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            self.scheduler.step()\n",
    "\n",
    "        return loss.item() * self.config.batch_acceleration\n",
    "\n",
    "    def log(self):\n",
    "        total_params = model_structure(self.model)\n",
    "        print(f\"Training parameters:\")\n",
    "        print(f\"Vocabulary size: {self.tokenizer.get_vocab_size()}\")\n",
    "        val_dataset_len, train_dataset_len = len(self.val_loader.dataset), len(\n",
    "            self.train_loader.dataset\n",
    "        )\n",
    "        print(f\"Context length: {self.config.model_args.seq_max_len}\")\n",
    "        print(f\"Dataset size: {val_dataset_len+train_dataset_len}\")\n",
    "        print(f\"Training set size: {train_dataset_len}\")\n",
    "        print(f\"Validation set size: {val_dataset_len}\")\n",
    "        nums_token = self.config.model_args.seq_max_len * train_dataset_len\n",
    "        print(f\"Tokens: {nums_token/1e6:.3f}M\")\n",
    "        print(f\"Model parameters: {total_params/1e6:.3f}M\")\n",
    "        print(\n",
    "            f\"Computational complexity: {(nums_token * total_params * 6)/1e12:.2f}TFLOPs * {self.config.epochs} = {(nums_token * total_params * 6 * self.config.epochs)/1e12:.2f}TFLOPs\"\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        gc.collect()\n",
    "        writer = SummaryWriter(log_dir=self.config.log_dir)\n",
    "        print(\"~~~Training started~~~\")\n",
    "\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Multi-GPU training: {torch.cuda.device_count()} GPUs\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        val_loss = 0\n",
    "        for epoch in range(self.config.epochs):\n",
    "            bar = tqdm(self.train_loader, unit=\"step\")\n",
    "            if epoch < self.start_epoch:\n",
    "                print(f\"Skipping already trained epoch: {epoch}\")\n",
    "                continue\n",
    "            elif epoch == self.start_epoch:\n",
    "                bar.update(self.start_step)\n",
    "\n",
    "            self.current_epoch = epoch\n",
    "            train_loss_sum = 0\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            for i, (train_inputs, train_targets) in enumerate(self.train_loader):\n",
    "                if i <= self.start_step and epoch == self.start_epoch:\n",
    "                    continue\n",
    "                self.current_step = i\n",
    "                loss = self._train_step(train_inputs, train_targets)\n",
    "\n",
    "                train_loss_sum += loss\n",
    "                self.train_loss_log.append((self.global_step, loss))\n",
    "                writer.add_scalar(\"Loss/train\", loss, self.global_step)\n",
    "                self.lr_log.append(\n",
    "                    (self.global_step, float(self.scheduler.get_last_lr()[0]))\n",
    "                )\n",
    "                self.global_step += 1\n",
    "                writer.add_scalar(\n",
    "                    \"LearningRate\",\n",
    "                    float(self.scheduler.get_last_lr()[0]),\n",
    "                    self.global_step,\n",
    "                )\n",
    "\n",
    "                if i % self.config.val_interval_step == 0:\n",
    "                    val_loss = self.validate()\n",
    "                    self.generate_test(\"AI is\")\n",
    "                    self.val_loss_log.append((self.global_step, val_loss))\n",
    "                    writer.add_scalar(\"Loss/val\", val_loss, self.global_step)\n",
    "\n",
    "                bar.update(1)\n",
    "                bar.postfix = f\"train_loss: {loss:.2f} test_loss: {val_loss:.2f} lr: {self.scheduler.get_last_lr()[0]:.2e}\"\n",
    "\n",
    "                if self.global_step % self.config.ckpt_interval_step == 0:\n",
    "                    val_loss = self.validate()\n",
    "                    self.val_loss_log.append((self.global_step, val_loss))\n",
    "                    ckpt_path = f\"{self.config.ckpt_save_dir.rsplit('.', 1)[0]}_epoch_{self.current_epoch}_step_{self.global_step}.pth\"\n",
    "                    self.save_checkpoint(ckpt_path, is_final=False)\n",
    "\n",
    "            bar.close()\n",
    "\n",
    "            val_loss = self.validate()\n",
    "            self.val_loss_log.append((self.global_step, val_loss))\n",
    "\n",
    "            self.generate_test()\n",
    "\n",
    "            self.save_checkpoint(\n",
    "                f\"{self.config.ckpt_save_dir.rsplit('.', 1)[0]}_epoch_{epoch}.pth\",\n",
    "                is_final=True,\n",
    "            )\n",
    "\n",
    "            test_text = self.generate_test(gen_len=10)\n",
    "            writer.add_text(\n",
    "                \"GeneratedText\", f\"epoch_{epoch}: {test_text}\", self.global_step\n",
    "            )\n",
    "\n",
    "            print(f\"Learning rate {self.scheduler.get_last_lr()}\")\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1}/{self.config.epochs}, avg_train_loss: {train_loss_sum/len(self.train_loader)}, avg_test_loss: {val_loss}\"\n",
    "            )\n",
    "\n",
    "        self.save_checkpoint(self.config.model_save_dir, is_final=True)\n",
    "\n",
    "    def validate(self) -> float:\n",
    "        self.model.eval()\n",
    "        val_loss_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in self.val_loader:\n",
    "                val_inputs = val_inputs.to(self.device)\n",
    "                val_targets = val_targets.to(self.device)\n",
    "                with torch.autocast(str(self.device), enabled=self.config.use_amp):\n",
    "                    val_output = self.model(val_inputs)\n",
    "                    loss = self.criterion(\n",
    "                        val_output.view(-1, self.config.model_args.vocab_size),\n",
    "                        val_targets.view(-1),\n",
    "                    )\n",
    "                val_loss_sum += loss.item()\n",
    "        return val_loss_sum / len(self.val_loader)\n",
    "\n",
    "    def generate_test(self, start: str = \"I\", gen_len: int = 25):\n",
    "        self.model.eval()\n",
    "        ans = self.generator.generate(\n",
    "            start_token=start, gen_seq_len=gen_len, print_out=False\n",
    "        )\n",
    "        ans = ans[len(start) :]\n",
    "        result = \"\".join(ans)\n",
    "        print(f\"(input){start}-> {result}\")\n",
    "        return result\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig, ax1 = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "        train_steps = [step for step, loss in self.train_loss_log]\n",
    "        train_losses = [loss for step, loss in self.train_loss_log]\n",
    "\n",
    "        val_steps = [step for step, loss in self.val_loss_log]\n",
    "        val_losses = [loss for step, loss in self.val_loss_log]\n",
    "\n",
    "        ax1.plot(train_steps, train_losses, label=\"Train Loss\")\n",
    "        ax1.plot(val_steps, val_losses, \"o-\", label=\"Test Loss\")\n",
    "        ax1.set_xlabel(\"Steps\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.legend(loc=\"upper left\")\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(\n",
    "            [step for step, _ in self.lr_log],\n",
    "            [float(value) for _, value in self.lr_log],\n",
    "            label=\"Learning Rate\",\n",
    "            color=\"c\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        ax2.set_ylabel(\"Learning Rate\")\n",
    "        ax2.tick_params(axis=\"y\")\n",
    "        ax2.legend(loc=\"upper right\")\n",
    "\n",
    "        plt.title(\"Train and Test Loss Curves with Learning Rate\")\n",
    "        plt.show()\n",
    "\n",
    "# The main execution code is commented out to avoid running when importing in notebook\n",
    "# if __name__ == \"__main__\":\n",
    "#     config = TrainingConfig()\n",
    "#     config_dict = asdict(config.model_args)\n",
    "#     with open(config.config_save_dir, \"w\") as f:\n",
    "#         json.dump(config_dict, f, indent=4)\n",
    "#     trainer = PreTrainer(config)\n",
    "#     trainer.log()\n",
    "#     trainer.train()\n",
    "#     trainer.plot_losses()\n",
    "#\n",
    "#     MAX_LEN = 10\n",
    "#     T = 0.8\n",
    "#     while True:\n",
    "#         start = input(\"In>>\")\n",
    "#         if start[:2] == \"T=\":\n",
    "#             T = float(start[2:])\n",
    "#             print(f\"T={T}\")\n",
    "#         else:\n",
    "#             print(\n",
    "#                 f\"T={T}\\n\"\n",
    "#                 + \"\".join(\n",
    "#                     trainer.generator.generate(\n",
    "#                         start_token=start,\n",
    "#                         gen_seq_len=MAX_LEN,\n",
    "#                         temperature=T,\n",
    "#                         frequency_penalty=10,\n",
    "#                         print_out=False,\n",
    "#                     )\n",
    "#                 )\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Code\n",
    "\n",
    "The following code is used to test if the notebook works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration created successfully\n",
      "Model parameters: d_model=256, n_layers=1\n",
      "Model arguments dictionary: {'d_model': 256, 'd_inner': 640, 'n_layers': 1, 'vocab_size': None, 'seq_max_len': 192, 'use_moe': False, 'n_heads': None, 'n_experts': None, 'n_experts_per_tok': None, 'd_conv': None, 'conv_bias': None, 'ffn_bias': False, 'attn_bias': True, 'd_head': 64, 'dropout': 0.1, 'init_std': 0.25, 'resid_pdrop': 0.1, 'resid_scale': 1.0, 'layer_scale': 1.0, 'use_deepnet_scaling': True}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel arguments dictionary:\u001b[39m\u001b[33m\"\u001b[39m, config_dict)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Print model structure\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model = \u001b[43mMyLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel created successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 349\u001b[39m, in \u001b[36mMyLM.__init__\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m    347\u001b[39m \u001b[38;5;28mself\u001b[39m.args = args\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m \u001b[38;5;28mself\u001b[39m.token_embedding = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[38;5;28mself\u001b[39m.blocks = nn.ModuleList(\n\u001b[32m    352\u001b[39m     [MyLMDecoderLayer(args) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args.n_layers)]\n\u001b[32m    353\u001b[39m )\n\u001b[32m    355\u001b[39m \u001b[38;5;28mself\u001b[39m.norm = RMSNorm(args.d_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Miniconda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:169\u001b[39m, in \u001b[36mEmbedding.__init__\u001b[39m\u001b[34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28mself\u001b[39m.scale_grad_by_freq = scale_grad_by_freq\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    170\u001b[39m         requires_grad=\u001b[38;5;129;01mnot\u001b[39;00m _freeze,\n\u001b[32m    171\u001b[39m     )\n\u001b[32m    172\u001b[39m     \u001b[38;5;28mself\u001b[39m.reset_parameters()\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "# Test the configuration creation\n",
    "config = TrainingConfig()\n",
    "print(\"Configuration created successfully\")\n",
    "print(f\"Model parameters: d_model={config.model_args.d_model}, n_layers={config.model_args.n_layers}\")\n",
    "\n",
    "# Print config dictionary\n",
    "config_dict = asdict(config.model_args)\n",
    "print(\"Model arguments dictionary:\", config_dict)\n",
    "\n",
    "# Print model structure\n",
    "model = MyLM(config.model_args)\n",
    "print(\"Model created successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
